## 預測變量越多越好嗎

### 變量越多總是會提高模型的擬合程度 

過度擬合的實例 overfitting：下面的數據是一組關於類人猿平均腦容量和平均體重的數據。


```{r introBayes10-01, cache=TRUE}
sppnames <- c( "afarensis", "africanus", "habilis", "boisei", 
               "rudolfensis", "ergaster", "sapiens")
brainvolcc <- c(438, 452, 612, 521, 752, 871, 1350)
masskg <- c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5)
d <- data.frame( species = sppnames, brain = brainvolcc, mass = masskg)
d
```

不同種類的猿人中，體重和腦容量呈現高度相關性並不稀奇。我們更加關心的是，當考慮了體重大小之後，是否某些種類的猿人的腦容量比我們預期的要大很多？常見的解決方案是用一個把體重作為預測變量，腦容量作為結果變量的簡單線性回歸模型來描述該數據。我們現看看該數據的散點圖：


```{r introBayes10-fig01, cache=TRUE, fig.width=6, fig.height=5,  fig.cap="Average brain volume in cubic centimeters against body mass in kilograms, for six hominin species. What model best describes the relationship between brain size and body size?", fig.align='center'}
# with(d, plot(mass, brain, 
#              xlab = "body mass (kg)", 
#              ylab = "brain volumn (cc)"))
# library(ggrepel)
ggthemr('greyscale')
d %>%
  ggplot(aes(x =  mass, y = brain, 
             label = species),
           ggtheme = theme_bw()) +
  geom_point(color = rangi2) +
  geom_text_repel(size = 5) +
  labs(x = "body mass (kg)",
       y = "brain volume (cc)") +
  xlim(30, 65) + 
  theme(
    axis.text = element_text(face = "bold", 
                               color = "black",
                               size = 13),
    axis.title =element_text(face = "bold", 
                               color = "black",
                               size = 15)
  )

```

接下來我們來建立一系列越來越複雜的模型。最簡單的模型就是線性回歸模型。在建立模型之前，先把體重變量標準化，然後把腦容量變量的單位縮放一下成爲一個範圍是 0-1 的變量：

```{r introBayes10-02, cache=TRUE}
d$mass_std <- (d$mass - mean(d$mass)) / sd(d$mass)
d$brain_std <- d$brain / max(d$brain)
d
```

我們想要建立的第一個模型是這樣的：


$$
\begin{aligned}
b_i & \sim \text{Normal}(\mu_i, \sigma) \\ 
\mu_i & = \alpha + \beta m_i \\
\alpha & \sim \text{Normal}(0.5, 1) \\
\beta & \sim \text{Normal}(0, 10) \\
\sigma & \sim \text{Log-Normal}(0, 1)
\end{aligned}
$$

```{r introBayes10-03, cache=TRUE}
m7.1 <- quap(
  alist(
    brain_std ~ dnorm( mu, exp(log_sigma) ), 
    mu <- a + b * mass_std, 
    a ~ dnorm( 0.5, 1 ), 
    b ~ dnorm( 0, 10 ), 
    log_sigma ~ dnorm( 0, 1 )
  ), data = d
)
precis(m7.1)
# compared with traditional OLS estimation
m7.1_OLS <- lm(brain_std ~ mass_std, data = d)
summary(m7.1_OLS)
```
下面的代碼計算該模型 `m7.1` 的 $R^2$。


```{r introBayes10-04, cache=TRUE}
set.seed(12)
s <- sim( m7.1 )
r <- apply(s, 2, mean) - d$brain_std
resid_var <- var2(r)
outcome_var <- var2(d$brain_std)
1 - resid_var / outcome_var
```

把上面的計算過程製作成一個函數，以便重複調用：

```{r introBayes10-05, cache=TRUE}
R2_is_bad <- function( quap_fit ){
  set.seed(12)
  s <- sim( quap_fit, refresh = 0)
  r <- apply(s, 2, mean) - d$brain_std
  1 - var2(r)/var2(d$brain_std)
}

R2_is_bad(m7.1)
```

接下來，我們把這個模型擴展開，讓它更加複雜一些，增加一個二次項，試圖提升模型擬合度。

$$
\begin{aligned}
b_i & \sim \text{Normal}(\mu_i, \sigma)\\ 
\mu_i & = \alpha + \beta_1 m_i + \beta_2 m_i^2 \\
\beta_j & \sim \text{Normal}(0, 10)  & \text{for } j = 1, 2\\
\sigma & \sim \text{Log-Normal}(0,1)
\end{aligned}
$$

```{r introBayes10-06, cache=TRUE}
m7.2 <- quap(
  alist(
    brain_std ~ dnorm( mu, exp(log_sigma) ), 
    mu <- a + b[1] * mass_std + b[2] * mass_std^2, 
    a ~ dnorm(0.5, 1) , 
    b ~ dnorm(0, 10), 
    log_sigma ~ dnorm(0, 1)
  ), data = d, start = list(b = rep(0, 2))
)
precis(m7.2, depth = 2)
```

接下來，我們頭腦發熱狂加多次項到模型中去看會發生什麼：


```{r introBayes10-07, cache=TRUE}
m7.3 <- quap(
  alist(
    brain_std ~ dnorm( mu, exp(log_sigma) ), 
    mu <- a + b[1] * mass_std + b[2] * mass_std^2 + 
      b[3] * mass_std^3, 
    a ~ dnorm(0.5, 1) , 
    b ~ dnorm(0, 10), 
    log_sigma ~ dnorm(0, 1)
  ), data = d, start = list(b = rep(0, 3))
)
m7.4 <- quap(
  alist(
    brain_std ~ dnorm( mu, exp(log_sigma) ), 
    mu <- a + b[1] * mass_std + b[2] * mass_std^2 + 
      b[3] * mass_std^3 + b[4] * mass_std^4, 
    a ~ dnorm(0.5, 1) , 
    b ~ dnorm(0, 10), 
    log_sigma ~ dnorm(0, 1)
  ), data = d, start = list(b = rep(0, 4))
)
m7.5 <- quap(
  alist(
    brain_std ~ dnorm( mu, exp(log_sigma) ), 
    mu <- a + b[1] * mass_std + b[2] * mass_std^2 + 
      b[3] * mass_std^3 + b[4] * mass_std^4 + 
      b[5] * mass_std^5, 
    a ~ dnorm(0.5, 1) , 
    b ~ dnorm(0, 10), 
    log_sigma ~ dnorm(0, 1)
  ), data = d, start = list(b = rep(0, 5))
)
m7.6 <- quap(
  alist(
    brain_std ~ dnorm( mu, exp(log_sigma) ), 
    mu <- a + b[1] * mass_std + b[2] * mass_std^2 + 
      b[3] * mass_std^3 + b[4] * mass_std^4 + 
      b[5] * mass_std^5 + b[6] * mass_std^6, 
    a ~ dnorm(0.5, 1) , 
    b ~ dnorm(0, 10), 
    log_sigma ~ dnorm(0, 1)
  ), data = d, start = list(b = rep(0, 6))
)
```

然後我們繪製上述每個模型給出的回歸線及其89%可信區間：

```{r introBayes10-fig02, cache=TRUE, fig.width=6, fig.height=5.5,  fig.cap="Linear model of increasing degree for the hominin data. The posterior mean in black, with 89% interval of the mean shaded. R^2 is diplayed.", fig.align='center'}
post <- extract.samples( m7.1 )
mass_seq <- seq( from = min(d$mass_std), to = max(d$mass_std), 
                 length.out = 100)
l <- link( m7.1, data = list(mass_std = mass_seq))
mu <- apply(l, 2, mean)
ci <- apply(l, 2, PI)

plot( brain_std ~ mass_std, data = d, 
      main = "m7.1: R^2 = 0.48", 
      bty="n", 
      col = rangi2, 
      xaxt = "n", 
      yaxt = "n",
      pch = 16,
      xlab = "body mass (kg)", 
      ylab = "brain volumn (cc)")
at <- c(-2, -1, 0, 1, 2)
labels <- at*sd(d$mass) + mean(d$mass)
at_y <- seq(0.2, 1, by = 0.1)
labels_y <- at_y * max(d$brain)
axis( side = 2, at = at_y, labels = round(labels_y, 1))
axis( side = 1, at = at, labels = round(labels, 1))
lines( mass_seq, mu)
shade(ci, mass_seq)

```

#### m7.2

```{r introBayes10-fig03, cache=TRUE, fig.width=6, fig.height=5.5,  fig.cap="Second-degree polynomial linear model of increasing degree for the hominin data. The posterior mean in black, with 89% interval of the mean shaded. R^2 is diplayed.", fig.align='center'}
post <- extract.samples( m7.2 )
mass_seq <- seq( from = min(d$mass_std), to = max(d$mass_std), 
                 length.out = 100)
l <- link( m7.2, data = list(mass_std = mass_seq))
mu <- apply(l, 2, mean)
ci <- apply(l, 2, PI)

plot( brain_std ~ mass_std, data = d, 
      main = "m7.2: R^2 = 0.53", 
      bty="n", 
      col = rangi2, 
      xaxt = "n", 
      yaxt = "n",
      pch = 16,
      xlab = "body mass (kg)", 
      ylab = "brain volumn (cc)")
at <- c(-2, -1, 0, 1, 2)
labels <- at*sd(d$mass) + mean(d$mass)
at_y <- seq(0.2, 1, by = 0.1)
labels_y <- at_y * max(d$brain)
axis( side = 2, at = at_y, labels = round(labels_y, 1))
axis( side = 1, at = at, labels = round(labels, 1))
lines( mass_seq, mu)
shade(ci, mass_seq)
```


#### m7.3

```{r introBayes10-fig04, cache=TRUE, fig.width=6, fig.height=5.5,  fig.cap="Third-degree polynomial linear model of increasing degree for the hominin data. The posterior mean in black, with 89% interval of the mean shaded. R^2 is diplayed.", fig.align='center'}
post <- extract.samples( m7.3 )
mass_seq <- seq( from = min(d$mass_std), to = max(d$mass_std), 
                 length.out = 100)
l <- link( m7.3, data = list(mass_std = mass_seq))
mu <- apply(l, 2, mean)
ci <- apply(l, 2, PI)

plot( brain_std ~ mass_std, data = d, 
      main = "m7.3: R^2 = 0.68", 
      bty="n", 
      col = rangi2, 
      xaxt = "n", 
      yaxt = "n",
      pch = 16,
      xlab = "body mass (kg)", 
      ylab = "brain volumn (cc)")
at <- c(-2, -1, 0, 1, 2)
labels <- at*sd(d$mass) + mean(d$mass)
at_y <- seq(0.2, 1, by = 0.1)
labels_y <- at_y * max(d$brain)
axis( side = 2, at = at_y, labels = round(labels_y, 1))
axis( side = 1, at = at, labels = round(labels, 1))
lines( mass_seq, mu)
shade(ci, mass_seq)
```


#### m7.4

```{r introBayes10-fig05, cache=TRUE, fig.width=6, fig.height=5.5,  fig.cap="Second-degree polynomial linear model of increasing degree for the hominin data. The posterior mean in black, with 89% interval of the mean shaded. R^2 is diplayed.", fig.align='center'}
post <- extract.samples( m7.4 )
mass_seq <- seq( from = min(d$mass_std), to = max(d$mass_std), 
                 length.out = 100)
l <- link( m7.4, data = list(mass_std = mass_seq))
mu <- apply(l, 2, mean)
ci <- apply(l, 2, PI)

plot( brain_std ~ mass_std, data = d, 
      main = "m7.4: R^2 = 0.82", 
      bty="n", 
      col = rangi2, 
      xaxt = "n", 
      yaxt = "n",
      pch = 16,
      xlab = "body mass (kg)", 
      ylab = "brain volumn (cc)")
at <- c(-2, -1, 0, 1, 2)
labels <- at*sd(d$mass) + mean(d$mass)
at_y <- seq(0.2, 1, by = 0.1)
labels_y <- at_y * max(d$brain)
axis( side = 2, at = at_y, labels = round(labels_y, 1))
axis( side = 1, at = at, labels = round(labels, 1))
lines( mass_seq, mu)
shade(ci, mass_seq)
```



#### m7.5

```{r introBayes10-fig06, cache=TRUE, fig.width=6, fig.height=5.5,  fig.cap="Second-degree polynomial linear model of increasing degree for the hominin data. The posterior mean in black, with 89% interval of the mean shaded. R^2 is diplayed.", fig.align='center'}
post <- extract.samples( m7.5 )
mass_seq <- seq( from = min(d$mass_std), to = max(d$mass_std), 
                 length.out = 100)
l <- link( m7.5, data = list(mass_std = mass_seq))
mu <- apply(l, 2, mean)
ci <- apply(l, 2, PI)

plot( brain_std ~ mass_std, data = d, 
      main = "m7.5: R^2 = 0.99", 
      bty="n", 
      col = rangi2, 
      xaxt = "n", 
      yaxt = "n",
      pch = 16,
      xlab = "body mass (kg)", 
      ylab = "brain volumn (cc)")
at <- c(-2, -1, 0, 1, 2)
labels <- at*sd(d$mass) + mean(d$mass)
at_y <- seq(0.2, 1, by = 0.1)
labels_y <- at_y * max(d$brain)
axis( side = 2, at = at_y, labels = round(labels_y, 1))
axis( side = 1, at = at, labels = round(labels, 1))
lines( mass_seq, mu)
shade(ci, mass_seq)
```



#### m7.6

```{r introBayes10-fig07, cache=TRUE, fig.width=6, fig.height=5.5,  fig.cap="Second-degree polynomial linear model of increasing degree for the hominin data. The posterior mean in black, with 89% interval of the mean shaded. R^2 is diplayed.", fig.align='center'}
post <- extract.samples( m7.6 )
mass_seq <- seq( from = min(d$mass_std), to = max(d$mass_std), 
                 length.out = 100)
l <- link( m7.6, data = list(mass_std = mass_seq))
mu <- apply(l, 2, mean)
ci <- apply(l, 2, PI)

plot( brain_std ~ mass_std, data = d, 
      main = "m7.6: R^2 = 1", 
      bty="n", 
      col = rangi2, 
      xaxt = "n", 
      yaxt = "n",
      pch = 16,
      ylim = c(-0.3, 1.5),
      xlab = "body mass (kg)", 
      ylab = "brain volumn (cc)")
at <- c(-2, -1, 0, 1, 2)
labels <- at*sd(d$mass) + mean(d$mass)
at_y <- seq(-0.3, 1.4, by = 0.1)
labels_y <- at_y * max(d$brain)
axis( side = 2, at = at_y, labels = round(labels_y, 1))
axis( side = 1, at = at, labels = round(labels, 1))
lines( mass_seq, mu)
shade(ci, mass_seq)
```
不難發現，當我們不斷地給模型增加多次項時，模型的擬合度，用 $R^2$ 表示的話，是越來越接近完美的。但是最完美的模型 `m7.6` 給出了瘋狂的曲線，它完美的預測了每一個觀察數據。但是你不能相信這個模型對嗎，因為它竟然告訴我們當體重在 58 KG 時，腦容量是小於零的。這是極端的過擬合現象，overfitting。

### 也不是預測變量越少越好

過度擬合的模型，它的問題在於可以完美的告訴我們已有數據內的關係 (within sample accuracy)，但是預測新數據的能力很差。與之相對應的，是擬合不足 (underfitting)。擬合不足的模型帶來的最大問題是，連已有的數據都無法準確地描述，更不要說用來預測了。


## 信息熵 information entropy

> The uncertainty contained in a probability distribution is the average log-probability of an event. 

翻譯過來就是，一個概率分佈所蘊含的不確定性，其實是該事件發生概率的對數均值取負數，這個概念被叫做**信息熵**。


$$
H(p) = - E \log (p_i) = - \sum_{i = 1}^n p_i \log(p_i)
$$

假如一個地方天氣只有晴天和下雨兩種，且各自發生的概率分別是 $p_1 = 0.3, p_2 = 0.7$，那麼我們認爲該地方天氣這一事件的信息熵是：

$$
H(p) = - (p_1\log(p_1) + p_2\log(p_2)) \approx 0.61
$$

```{r introBayes10-08, cache=TRUE}
p <- c(0.3, 0.7)
-sum(p*log(p))
```

假如我們搬家到阿布扎比。那麼我們知道這裏鮮少下雨，雨天和晴天的概率可能就是 $p_1 = 0.01, p_2 = 0.99$。那麼在阿布扎比的天氣，其信息熵（或者簡單說叫做天氣變化的不確定性）就是約等於0.06：


```{r introBayes10-09, cache=TRUE}
p <- c(0.01, 0.99)
-sum(p*log(p))
```

也就是說這裏天氣的不確定，相比一個降雨概率是30%的地方來說是相對很低的。當然，假如我們增加天氣的不同種類，例如下雪天的概率。通常信息熵會增加，也就是天氣的不確定性會增加。例如我們假設晴天，下雨，降雪的概率分別是 $p_1 = 0.7, p_2 = 0.15, p_3 = 0.15$，那麼其天氣的信息熵就約等於0.82：


```{r introBayes10-10, cache=TRUE}
p <- c(0.7, 0.15, 0.15)
-sum(p*log(p))
```



### 從信息熵到精確度 From entropy to accuracy

> **Divergence**: The additional uncertainty induced by using probabilities from one distribution to describe another distribution. 

離散度，又被叫做 Kullback-Leibler 離散度。如果某個事件的真實概率分佈 (True distribution of events) 是 $p_1 = 0.3, p_2 = 0.7$。如果我們使用的模型計算獲得的事後概率分佈是 $q_1 = 0.25, q_2 = 0.75$，那麼由於模型造成的額外的不確定性 (additional uncertainty) 該如何測量呢？答案是要根據上面定義的信息熵來做比較：


$$
D_{\text{KL}} = \sum_i p_i(\log(p_i) - \log(q_i)) = \sum_i p_i \log\left( \frac{p_i}{q_i} \right)
$$

也就是說，一個模型的離散程度是模型給出的概率分佈和我們希望測量的目標概率分佈之間的對數概率差的平均值 [the divergence is the average difference in log probability between the target (p) and model (q)]。這個離散度的定義其實是兩個概率分佈之間的對數概率（兩個信息熵）之差而已。


