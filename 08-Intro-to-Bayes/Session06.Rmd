這裡的簡單筆記來自經典教學書 [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/)。


## 為什麼正（常）態分佈是正常的 why normal distribution is normal?


> Keep in mind that using a model is not equivalent to swearing an oath on it. The golem is your servant, not the other way around.
> ~ Richard McElreath


概率分佈如果是離散型的，如二項分佈，描述它的分佈函數被叫做概率質量函數 (probability mass functions)。當概率分佈是連續型的，如正（常）態/高斯分佈，描述它的分佈函數被叫做概率密度函數 (probability density functions)。概率密度本身的取值是可以大於1的，如：

```{r}
dnorm(0, 0, 0.1) # mean = 0, sd = 0.1
```

是讓 R 幫助我們計算 $p(0 | 0, (\sqrt{0.1})^2)$ 的取值。其結果差不多要等於4。這並沒有出錯，**概率密度**的概念其實是累積概率的增長速率 (the rate of change in cumulative probability)。所以在概率密度函數的平滑曲線上，累積概率增加的越快的區間，它的增長速率，也就是概率密度很容易超過1。但是，概率密度函數的曲線下面積，則永遠不可能超過1。

## 身高的高斯模型 Gaussian model of height


```{r introBayes06-01, cache=TRUE, message=FALSE}
library(rethinking)
data("Howell1")

d <- Howell1
str(d)

precis(d)

# filter out non-adults

d2 <- d[ d$age >= 18, ]

```


寫下身高的模型，

$$
\begin{aligned}
h_i & \sim \text{Normal}(\mu, \sigma) &\text{     [Likelihood]} \\
\mu & \sim \text{Normal}(178, 20)     &\text{     [prior for } \mu] \\
\sigma & \sim \text{Uniform}(0, 50)          &\text{     [prior for } \sigma]\\
\end{aligned}
$$

觀察一下我們設定的先驗概率分佈的形狀：

```{r introBayes06-02, cache=TRUE, message=FALSE,fig.asp=.7, fig.width=6,  fig.cap='The shape for prior distribution of the mean.', fig.align='center', out.width='90%'}
curve( dnorm( x, 178, 20), from = 100, to = 250, xlab = ~mu)
```

```{r introBayes06-03, cache=TRUE, message=FALSE,fig.asp=.7, fig.height=3.5, fig.width=4, fig.cap='The shape for prior distribution of the standard deviation.', fig.align='center', out.width='90%'}

curve( dunif( x, 0, 50), from = -10, to = 60, xlab = ~sigma)
```

當你選定了參數 $h, \mu, \sigma$ 的先驗概率分佈時，你其實同時選定了他們的聯合分佈 (joint distribution)。從這個聯合先驗概率分佈中採集一些樣本，可以輔助我們判斷選擇這樣的先驗概率分佈是否是合適/不壞的。

```{r introBayes06-04, cache=TRUE, message=FALSE,fig.asp=.7, fig.height=3, fig.width=4, fig.cap='The shape for samplings from prior distribution of the height generated from the joint distrition chosen for the mean and standard deviation.', fig.align='center', out.width='90%'}
sample_mu <- rnorm(10000, 178, 20)
sample_sigma <- runif(10000, 0, 50)
prior_h <- rnorm(10000, sample_mu, sample_sigma)
dens( prior_h )
```


看起來身高的先驗概率分佈似乎合乎常理，可以暫且認為均值和標準差的先驗概率選擇並不太糟糕。


當你刻意去選擇更加寬泛的先驗概率分佈給均值時，可能導致不合理的數據出現。如下圖 \@ref(fig:introBayes06-05) 中顯示的那樣，當刻意給身高的均值 $\mu$ 賦予更加“不含信息”，或者模糊的先驗概率 $\text{Normal}(178, 100)$，時給出的身高的先驗概率分佈中出現了不少身高為負數的情況，這就是不合理的先驗概率分佈。

```{r introBayes06-05, cache=TRUE, message=FALSE, fig.height=3, fig.width=4, fig.cap='The shape for samplings from prior distribution of the height generated from the joint distrition chosen for the mean and standard deviation, with flatter and less information prior for the mean (mu ~ N(178, 100)).', fig.align='center', out.width='90%', fig.asp=.7}
sample_mu <- rnorm(10000, 178, 100)
prior_h <- rnorm(10000, sample_mu, sample_sigma)
dens( prior_h )
```


模型的貝葉斯定理表達式：


$$
\text{Pr}(\mu, \sigma | h) = \frac{\prod_i \text{Normal}(h_i|\mu, \sigma)\text{Normal}(\mu | 178, 20)\text{Uniform}(\sigma|0,50)}{\int\int \prod_i \text{Normal}(h_i|\mu, \sigma)\text{Normal}(\mu | 178, 20)\text{Uniform}(\sigma|0,50) d\mu d\sigma}
$$


### 小方格估計法近似事後概率分佈


```{r  introBayes06-06, cache=TRUE}
# 在 150 - 160 之間產生100個數據區間
mu_list <- seq( from = 150, to = 160, length.out = 100)

# 在 7-9 之間產生 100 個數據區間
sigma_list <- seq( from  = 7, to = 9, length.out = 100)

# 用上面生成的方差和均值交叉做出 10000 個均值和方差的數據格子
post <- expand.grid( mu = mu_list, sigma = sigma_list)

# 在對數尺度上用加法運算，會節約計算成本
post$LL <- sapply( 1:nrow(post), function(i) sum(
  dnorm( d2$height, post$mu[i], post$sigma[i], log = TRUE)
))

# 計算取對數下的事後概率分佈的分母部分 
post$prod <- post$LL + 
  dnorm( post$mu, 178, 20, log = TRUE) + 
  dunif(post$sigma, 0, 50, log = TRUE)

# Scale all of the log-products by the maximum log-product

post$prob <- exp( post$prod - max(post$prod))

```


可以繪製簡單的事後均值，事後標準差的等高圖分佈，登高線本身使用上面代碼計算的身高的相對事後概率分佈。

```{r introBayes06-07, fig.asp=.7, fig.width=6,  fig.cap='Simple contour plot of the posterior means and standard deviations of height with relative posterior probability distribution. ', fig.align='center', out.width='90%', cache=TRUE}
contour_xyz( post$mu, post$sigma, post$prob)
```

```{r introBayes06-08, fig.asp=.7, fig.width=6,  fig.cap='Simple heat map plot of the posterior means and standard deviations of height with relative posterior probability distribution. ', fig.align='center', out.width='90%', cache=TRUE}
image_xyz(post$mu, post$sigma, post$prob)
```


### 從計算獲得的事後概率分佈中採樣


```{r introBayes06-09, fig.asp=.7, fig.width=6,  fig.cap='Samples from the posterior distribution for the heights data.', fig.align='center', out.width='90%', cache=TRUE}

sample.rows <- sample(1 : nrow(post), size = 10000, replace = TRUE,
                      prob = post$prob)
sample.mu <- post$mu[ sample.rows ]
sample.sigma <- post$sigma[ sample.rows]

plot(sample.mu, sample.sigma, cex = 0.7, pch = 16, 
     col = col.alpha(rangi2, 0.2))
```

```{r}
dens(sample.mu)
dens(sample.sigma)
PI(sample.mu)
PI(sample.sigma)
```

### 使用二次方程近似法


使用R語言定義我們的身高模型：


$$
\begin{aligned}
h_i & \sim \text{Normal}(\mu, \sigma) &\text{     [Likelihood]} \\
\mu & \sim \text{Normal}(178, 20)     &\text{     [prior for } \mu] \\
\sigma & \sim \text{Uniform}(0, 50)          &\text{     [prior for } \sigma]\\
\end{aligned}
$$


```{r introBayes06-10, cache=TRUE, message=FALSE}
flist <- alist(
  height ~ dnorm( mu, sigma ), 
  mu     ~ dnorm( 178, 20 ), 
  sigma  ~ dunif( 0 , 50 )
)

m4.1 <- quap( flist, data = d2)

precis( m4.1 )
```


繪製該數據中身高曲線之間的關係圖：


```{r introBayes06-11, fig.height=4, fig.asp=.7, fig.width=4.5, fig.cap='Plot adults height against weight.', fig.align='center', out.width='90%', cache=TRUE} 
# plot(d2$height, d2$weight)
ggplot(data = d2, 
       aes(x = weight, y = height)) +
  geom_point(shape = 1, size = 2) +
  theme_bw() +
  theme(panel.grid = element_blank())
```

如何使用身高來做體重的預測變量，從而建立一個簡單線型回歸模型？


$$
\begin{aligned}
h_i & \sim \text{Normal}(\mu_i, \sigma)  & [\text{likelihood}]\\
\mu_i & = \alpha + \beta (x_i - \bar{x}) & [\text{linear model}]\\
\alpha & \sim \text{Normal}(178, 20)    & [\text{prior for } \alpha]\\
\beta & \sim \text{Normal}(0, 10)       & [\text{prior for } \beta]\\   
\sigma & \sim \text{Uniform}(0, 50)     & [\text{prior for } \sigma]
\end{aligned}
$$

#### 關於 $\beta$ 的先驗概率 Priors

代碼來自書中(P95 Rcode 4.38)：

```{r introBayes06-12, cache=TRUE, fig.asp=.7, fig.width=6, fig.cap='Prior predictive simulation for the height and weight model. Simulation using the beta ~ N(0, 10) prior.', fig.align='center', out.width='90%'}
set.seed(2971)
N <- 100 #n number of simulation lines

a <- rnorm( N, 178, 20)
b <- rnorm( N, 0, 10)

plot( NULL, xlim = range(d2$weight) , ylim = c(-100, 400), 
      xlab = "weight", ylab = "height")
abline( h = 0, lty = 2)
abline( h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dnorm(0, 10)" )

xbar <- mean(d2$weight)

for ( i in 1:N) curve( a[i] + b[i]*(x - xbar) ,
  from = min(d2$weight), to = max(d2$weight), add = TRUE, 
  col = col.alpha("black", 0.2)
  )

```


相同圖形使用 tidyverse 和 ggplot2 來製作(參考 [Statistical rethinking with brms, ggplot2, and the tidyverse: Second edition](https://bookdown.org/content/4857/geocentric-models.html#a-gaussian-model-of-height)：

```{r introBayes06-13, cache=TRUE, fig.asp=.7, fig.width=6,  fig.cap='Prior predictive simulation for the height and weight model. Simulation using the beta ~ N(0, 10) prior.', fig.align='center', out.width='90%'}
set.seed(4)
# how many lines would you like?
n_lines <- 100

lines <-
  tibble(n = 1:n_lines,
         a = rnorm(n_lines, mean = 178, sd = 20),
         b = rnorm(n_lines, mean = 0,   sd = 10)) %>% 
  tidyr::expand(tidyr::nesting(n, a, b), weight = range(d2$weight)) %>% 
  mutate(height = a + b * (weight - mean(d2$weight)))

head(lines)

lines %>% 
  ggplot(aes(x = weight, y = height, group = n)) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, size = 1/3) +
  geom_line(alpha = 1/10) +
  coord_cartesian(ylim = c(-100, 400)) +
  ggtitle("b ~ dnorm(0, 10)") +
  theme_classic()
```

可見這樣的身高作為先驗概率明顯是不合理的，因為有大量的數據低於 0 cm，甚至於大於人類的極限身高272cm。如何將這個關於身高的先驗概率合理化呢？解決方法的一種是使用對數正（常）態分佈。對數正（常）態分佈，其實就是指，一組取了對數之後的數值服從正（常）態分佈。

$$
\beta \sim \text{Log-Normal}(0, 1)
$$

```{r introBayes06-14, cache=TRUE, fig.asp=.7, fig.width=6,  fig.cap='The density shape for lognormal (0,1), it is the distribution whose logarithm is normally distributed.', fig.align='center'}
# b <- rlnorm(10000, 0, 1)
# dens(b, xlim = c(0, 5), adj = 0.1)
set.seed(4)
tibble(b = rlnorm(10000, mean = 0, sd = 1)) %>% 
  ggplot(aes(x = b)) + 
  geom_density(fill = "grey70") +
  coord_cartesian(xlim = c(0, 5)) +
  theme_classic()
```

我們還可以把採集的對數正（常）態分佈樣本取對數，和標準正（常）態分佈做一個圖形上的比較。

```{r introBayes06-15, cache=TRUE, fig.height=6, fig.width=4.5, fig.cap='The density shape for log(lognormal (0, 1)) and normal(0, 1), they are the same.', fig.align='center'}
set.seed(4)

tibble(rnorm = rnorm(100000, mean = 0, sd = 1),
       `log(rlognorm)` = log(rlnorm(100000, mean = 0, sd = 1))) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(x = value)) + 
  geom_density(fill = "grey70") + 
  coord_cartesian(xlim = c(-3, 3)) + 
  theme_classic() + 
  facet_wrap( ~ name, nrow = 2)

```



使用對數正（常）態分佈作為 $\beta$ 的先驗概率分佈，試試看它給出的身高預測範圍大致是怎樣的。


```{r introBayes06-16, cache=TRUE, fig.asp=.7, fig.width=6,  fig.cap="Prior predictive simulation for the height and weight model. Simulation using the beta ~ log-normal(0, 1) prior, within much reasonable human ranges.", fig.align='center'}
set.seed(2971)
N <- 100 #n number of simulation lines

a <- rnorm( N, 178, 20)
# b <- rnorm( N, 0, 10)
b <- rlnorm( N, 0, 1)

plot( NULL, xlim = range(d2$weight) , ylim = c(-100, 400), 
      xlab = "weight", ylab = "height")
abline( h = 0, lty = 2)
abline( h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dlognorm(0, 1)" )

xbar <- mean(d2$weight)

for ( i in 1:N) curve( a[i] + b[i]*(x - xbar) ,
  from = min(d2$weight), to = max(d2$weight), add = TRUE, 
  col = col.alpha("black", 0.2)
  )
```

使用 ggplot2 做一邊相同的事。

```{r introBayes06-17, cache=TRUE, fig.asp=.7, fig.width=6,  fig.cap="Prior predictive simulation for the height and weight model. Simulation using the beta ~ log-normal(0, 1) prior, within much reasonable human ranges.", fig.align='center'}
# make a tibble to annotate the plot
Text <-
  tibble(weight = c(34, 43),
         height = c(0 - 25, 272 + 25),
         label  = c("Embryo", "World's tallest person (272 cm)"))

# simulate
set.seed(4)

tibble(n = 1:n_lines,
       a = rnorm(n_lines, mean = 178, sd = 20),
       b = rlnorm(n_lines, mean = 0, sd = 1)) %>% 
  tidyr::expand(tidyr::nesting(n, a, b), weight = range(d2$weight)) %>% 
  mutate(height = a + b * (weight - mean(d2$weight))) %>% 
  
  # plot
  ggplot(aes(x = weight, y = height, group = n)) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, size = 1/3) +
  geom_line(alpha = 1/10) +
  annotate(geom = "text", 
           x = c(34, 43), y = c(-25, 297), 
           label = c("Embryo", "World's tallest person (272 cm)"),
           size = 5) +
  coord_cartesian(ylim = c(-100, 400)) +
  ggtitle("log(b) ~ dnorm(0, 1)") +
  theme_classic()
```

> A serious problem in contemporary applied statistics is "p-hacking," the practice of adjusting the model and the data to achieve a desired result.
> The desired result is usually a p-value less than 5%.
>
> `r tufte::quote_footer('--- [Richard McElreath](https://xcelab.net/rm/)')`


於是我們現在把修改了 $\beta$ 的先驗概率分佈之後的模型定義：


$$
\begin{aligned}
h_i & \sim \text{Normal}(\mu_i, \sigma)  & [\text{likelihood}]\\
\mu_i & = \alpha + \beta (x_i - \bar{x}) & [\text{linear model}]\\
\alpha & \sim \text{Normal}(178, 20)    & [\text{prior for } \alpha]\\
\beta & \sim \text{Log-Normal}(0, 1)       & [\text{prior for } \beta]\\   
\sigma & \sim \text{Uniform}(0, 50)     & [\text{prior for } \sigma]
\end{aligned}
$$

於是用這個模型來獲取我們需要的事後概率分佈的方法就顯而易見了：

```{r introBayes06-18, cache=TRUE}
# define the average weight, x-bar
xbar <- mean(d2$weight)

# fit model

m4.3 <- quap(
  alist(
    height ~ dnorm( mu, sigma ),
    mu <- a + b * (weight - xbar),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1), 
    sigma ~ dunif( 0, 50 )
  ), data = d2
)

precis(m4.3)
```


```{r introBayes06-19, cache=TRUE, message=FALSE}

d2 <- d2 %>% 
  mutate(weight_c = weight - mean(weight))

b4.3 <- 
  brm(data = d2, 
      family = gaussian, 
      height ~ 1 + weight_c,
      prior = c(prior(normal(178, 20), class = Intercept), 
                prior(lognormal(0, 1), class = b), 
                prior(uniform(0, 50), class = sigma)), 
      iter = 28000, warmup = 27000, chains = 4, cores = 4,
      seed = 4)

b4.3
```

觀察各個參數事後樣本之間的協方差 (covariances)，幾乎可以忽略不計。

```{r introBayes06-20, cache=TRUE, message=FALSE}
round( vcov(m4.3), 3)

posterior_samples(b4.3) %>%
  dplyr::select(-lp__) %>%
  cov() %>%
  round(digits = 3)
```


```{r introBayes06-21, cache=TRUE, fig.asp=.7, fig.width=6,  fig.cap='Both marginal posteriors and the covariance.', fig.align='center', out.width='90%'}
pairs(m4.3)
```

```{r introBayes06-22, cache=TRUE, fig.asp=.7, fig.width=6,  fig.cap='Both marginal posteriors and the covariance.', fig.align='center', out.width='90%'}
pairs(b4.3)
```