> The most newsworthy scientific studies are the least trustworthy. Maybe popular topics attract more and worse researchers, like flies drawn to the smell of honey?
> ~ Richard McElreath


Berkson's paradox, 又被叫做是選擇性扭曲現象（selection-distortion effect）。

```{r introBayes09-fig01, cache=TRUE, fig.width=6, fig.height=5,  fig.cap="Why the most newsworthy studies might be least trustworthy. 200 research proposals are ranked by combined trustworthiness and news worthiness. The top 10% are selected for funding. While there is no correlation before selection, the two criteria are strongly negatively correlated after selection. The correlation here is -0.77.", fig.align='center'}
set.seed(1914)
N <- 200 # num grant proposals
p <- 0.1 # proportion to select

# uncorrelated newsworthiness and trustworthiness
nw <- rnorm(N)
tw <- rnorm(N)

# select top 10 of combined scores
s <- nw + tw # total score
q <- quantile( s, 1-p ) # top 10% threshold

selected <- ifelse( s >= q, TRUE, FALSE) 
cor(tw[selected], nw[selected])
proposal <- data.frame(nw, tw, selected)
with(proposal, plot(nw[!selected], tw[!selected], col=c("black"),
                    xlim = c(-3,3.5), ylim = c(-3.5,3), 
                    bty="n", 
                    xlab = "newsworthiness", 
                    ylab = "trustworthiness"))
points(proposal$nw[proposal$selected], 
       proposal$tw[proposal$selected], 
       col = c("blue"), 
       pch = 16)
abline(lm(proposal$nw[proposal$selected] ~ proposal$tw[proposal$selected]), 
       lty = 2, lwd = 2, col = c("blue"))
text(1, -2.8, "rejected")
text(2, 2.5, "selected", col = c("blue"))
```

## 多重共線性問題 multicollinearity

多重共線性，通常當模型的預測變量之間有較強的相互關係的時候會出現。它造成的結果是你的模型給出的事後概率分佈會表現的似乎和任何一個預測變量之間都沒什麼關係，即便事實上其中的一個甚至幾個都可能和結果變量存在著相互依賴的關係。這樣的模型對於研究目的是使用模型來做預測的情形下沒有什麼本質的影響。

想像一下我們想使用一個人的腿長度來預測他/她的身高。你覺得模型中同時放入左右兩條腿的長度作為預測變量的話，事情會變成怎樣的呢？

下面的代碼是通過計算機模擬生成100個人的身高和腿長度。


```{r introBayes09-01, cache=TRUE}
N <- 100                        # number of individuals 
set.seed(909)
height <- rnorm(N, 10, 2)       # sim total height for each
leg_prop <- runif(N, 0.4, 0.5)  # leg as proportion of height 
leg_left <- leg_prop * height + # sim left leg as proportion + error
  rnorm( N, 0, 0.02 ) 
leg_right <- leg_prop * height + # sim right leg as proportion + error
  rnorm( N, 0, 0.02 )
                                  # combine into data frame
d <- data.frame(height, leg_left, leg_right)
head(d)
```

如果我們同時使用兩腿的長度作為預測身高的變量建立簡單線性回歸模型的話，我們會期待獲得怎樣的結果？從生成數據的過程我們已知平均地，腿長度佔身高的比例是45%。所以我們其實會期待腿長度的回歸係數應該在 $10/4.5 \approx 2.2$ 左右。但事實是怎樣呢？


```{r introBayes09-02, cache=TRUE}
m6.1 <- quap(
  alist(
    height ~ dnorm( mu, sigma ), 
    mu <- a + bl * leg_left + br * leg_right, 
    a ~ dnorm( 10, 100 ),
    bl ~ dnorm(2, 10), 
    br ~ dnorm(2, 10), 
    sigma ~ dexp( 1 )
  ), data = d
)
precis(m6.1)
```

左右腿的數據同時放到一個模型裡給出的結果似乎是令人困惑的。


```{r introBayes09-fig02, cache=TRUE, fig.width=6, fig.height=5,  fig.cap="If both legs have almost identical lengths, and height is so strongly associated with leg length, then why is this posterior distribution so weird?", fig.align='center'}
plot(precis(m6.1))
```

我們看模型 `m6.1` 給出的 `bl, br` 的事後聯合分佈 (joint posterior distribution)：

```{r introBayes09-fig03, cache=TRUE, fig.width=6, fig.height=5,  fig.cap="Posterior distribution of the association of each leg with hegiht, from model m6.1. Since both variables contain almost identical information, the posterior is a narrow ridge of negatively correlated values.", fig.align='center'}
post <- extract.samples(m6.1)
plot(bl ~ br, post, col = col.alpha(rangi2, 0.1),
     pch = 16)
```

如圖 \@ref(fig:introBayes09-fig03) 顯示的那樣，當 `bl` 很大時，`br` 就很小，反之亦然。


```{r introBayes09-fig04, cache=TRUE, fig.width=6, fig.height=5,  fig.cap="The posterior distribution of the sum of the two parameters is cnetered on the proper association of eight leg with height.", fig.align='center'}
sumblbr <- post$bl + post$br
dens(sumblbr, col = rangi2, lwd = 2, xlab = "sum of bl and br")
```


於是我們知道我們應該從模型中去掉其中一個腿的信息，從而獲得正確的模型和計算結果：


```{r  introBayes09-03, cache=TRUE}
m6.2 <- quap(
  alist(
    height ~ dnorm( mu, sigma ), 
    mu <- a + bl * leg_left, 
    a ~ dnorm( 10, 100 ), 
    bl ~ dnorm(2, 10), 
    sigma ~ dexp( 1 )
  ), data = d
)

precis(m6.2)
```


### 哺乳動物奶質量數據中的共線性

重新打開哺乳動物奶質量數據。我們看其中的含脂肪百分比和含乳糖百分比這兩個變量。把他們標準化：

```{r introBayes09-04, cache=TRUE}
data(milk)
d <- milk
d$K <- standardize( d$kcal.per.g )
d$F <- standardize( d$perc.fat )
d$L <- standardize( d$perc.lactose )
```

下面的模型使用標準化的脂肪百分比和乳糖百分比兩個變量作為預測變量來預測奶的能量密度：


```{r introBayes09-05, cache=TRUE}
# kcal.per.g regressed on perc.fat
m6.3 <- quap(
  alist(
    K ~ dnorm( mu, sigma ),
    mu <- a + bF * F, 
    a ~ dnorm( 0, 0.2 ), 
    bF ~ dnorm( 0, 0.5 ), 
    sigma ~ dexp(1)
  ), data = d
)

# kcal.per.g regressed on perc.lactose
m6.4 <- quap(
  alist(
    K ~ dnorm( mu, sigma ), 
    mu <- a + bL * L, 
    a ~ dnorm( 0, 0.2 ), 
    bL ~ dnorm( 0, 0.5 ), 
    sigma ~ dexp(1)
  ), data =  d
)


precis( m6.3 )
precis( m6.4 )
```

當單獨使用其中之一作為能量密度的預測變量時，我們發現他們各自的回歸係數似乎互相成鏡像數據，一個是正的，另一個是負的。而且二者的回歸係屬的事後概率分佈都很精確，我們認為這兩個單獨變量都是可以用來預測奶能量密度的極佳預測變量。因為脂肪百分比越高，能量密度越高，反之，乳糖含量比例越高，那麼能量密度則越低。我們來看把他們兩個同時加入模型中會發生什麼現象：

```{r introBayes09-06, cache=TRUE}
m6.5 <- quap(
  alist(
    K ~ dnorm( mu, sigma ), 
    mu <- a + bF * F + bL * L, 
    a ~ dnorm( 0, 0.2 ), 
    bF ~ dnorm( 0, 0.5 ), 
    bL ~ dnorm( 0, 0.5 ), 
    sigma ~ dexp( 1 )
  ), data = d
)
precis( m6.5 )
```

你看現在 `m6.5` 模型中同時加入了脂肪百分比，和乳糖百分比的兩個變量。都比單獨使用時給出的回歸係屬更接近 0。而且各自的事後概率分佈的標準差都比單獨使用時大了許多（幾乎兩倍）。這並非是來自計算機模擬的數據，而是真正現實中存在的奶製品測量之後的數據。脂肪百分比和乳糖百分比二者之間存在的很強的互相預測的關係。我們從他們的三點圖可以看出其中的奧妙：


```{r introBayes09-fig05, cache=TRUE, fig.width=6, fig.height=5,  fig.cap="A pairs plot of the total energy, percent fat, and percent lactose variables from the primate milk data. Percent fat and percent lactose are strongly negatively correlated with one another, providing mostly the same information. ", fig.align='center'}
pairs( ~ kcal.per.g + perc.fat + perc.lactose, data = d, 
       col = rangi2)
```


## 治療後偏倚 post-treatment bias

```{r introBayes09-07, cache=TRUE}
set.seed(71)
 # number of plants 
N <- 100

# simulate initial heights 
h0 <- rnorm(N, 10, 2)

# assign treatments and simulate fungus and growth
treatment <- rep(0:1, each = N/2)
fungus <- rbinom( N, size = 1, prob = 0.5 - treatment * 0.4)
h1 <- h0 + rnorm( N, 5 - 3*fungus )

# compose a clean data frame
d <- data.frame( h0 = h0, h1 = h1, treatment = treatment, fungus = fungus)
head(d)
precis(d)
```

### 設定模型

$$
\begin{aligned}
h_{1,i} & \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i & = h_{0,i} \times p
\end{aligned}
$$

其中，

- $h_{0,i}$ 是在時間 $t = 0$ 時的植物高度；
- $h_{1,i}$ 是在時間 $t = 1$ 時植物的高度；
- $p$ 是比例係數，也就是 $h_{1,i}$ 和 $h_{0,i}$ 之間的比值，$p = \frac{h_{1,i}}{h_{0,i}}$。如果 $p = 1$ 說明在時間 $t = 1$ 時植物並沒有比在時間 $t = 0$ 時有長高。

這裡我們對 $p$ 使用的先驗概率分佈，應該會集中在 1 的附近，因為無信息表示我們認為植物的高度不會隨時間發生變化。但是這個比例 $p$ 不能為負數。因為它是一個值和另一個值的比值。我們之前使用過相似特質的先驗概率分佈，也就是對數正（常）態分佈（Log-Normal distribution）：

$$
\beta \sim \text{Log-Normal}(0, 0.25)
$$

看看這個先驗概率分佈的密度曲線是什麼樣子：


```{r  introBayes09-fig06, cache=TRUE, fig.width=6, fig.height=5,  fig.cap="Distribution density funciton of Log-Normal(0,0.25)", fig.align='center'}
sim_p <- rlnorm(10000, 0, 0.25)
precis(sim_p)
dens( sim_p, xlim = c(0,3), adj = 0.1)
```


也就是說，我們給出的這個先驗概率分佈認為，植物在不同時間點之間的生長比例範圍在 0.67 和 1.49 之間，也就是要麼縮水33%，或者最多長高50%。


建立該模型：

```{r introBayes09-08, cache=TRUE}
m6.6 <- quap(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- h0 * p ,
    p ~ dlnorm( 0, 0.25 ),
    sigma ~ dexp(1)
  ), data  = d
)
precis(m6.6)
```

$p$ 的事後概率分佈均值是 1.43，也就是預估平均每單位時間植物會長高大約 40%。接下來如果加入另外兩個變量，治療組，和是否出現菌落。我們會把這兩個變量對植物施加的影響使用線性回歸模型的方式加到 $p$ 上去：


$$
\begin{aligned}
h_{1, i} & \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i  & = h_{0,i} \times p \\ 
p     & = \alpha + \beta_T T_i + \beta_F F_i \\
\alpha  & \sim \text{Log-Normal}(0, 0.25) \\
\beta_T & \sim \text{Normal}(0, 0.5) \\ 
\beta_F & \sim \text{Normal}(0, 0.5) \\ 
\sigma  & \sim \text{Exponential}(1)
\end{aligned}
$$

上述模型的R代碼如下：

```{r introBayes09-09, cache=TRUE}
m6.7 <- quap(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- h0 * p , 
    p <- a + bt * treatment + bf * fungus, 
    a ~ dlnorm( 0, 0.2 ),
    bt  ~ dnorm(0, 0.5), 
    bf  ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)
precis(m6.7)
```

這裡似乎在說，治療本身對植物生長速度並無效果，但是有菌落卻對生長比例造成了負影響。可是我們明明知道菌落是否存在，是取決於治療本身的，也就是菌落是治療對土壤造成的結果之一。上述模型似乎在告訴我們，當我們知道了治療造成的結果之一 -- 是否有菌落，那麼治療本身對植物生長比例的影響就消失了。正確的模型是，我們應該把菌落這個變量從模型中拿掉，從而尋找治療對植物生長率的效果：


```{r  introBayes09-10, cache=TRUE}
m6.8 <- quap(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- h0 * p, 
    p <- a + bt * treatment, 
    a ~ dlnorm(0, 0.25),
    bt ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)
precis(m6.8)
```

上述的分析過程和結果告訴我們，如果我們把由於治療本身造成的結果之一也錯誤地放進預測變量中的話，治療本身的效果會消失。

這些變量之間的關係還可以用下面的DAG圖來輔助理解：


```{r  introBayes09-fig07, cache=TRUE, fig.width=8, fig.height=1,  fig.cap="The DAG of the fungus and treatment effect on the grow of plant.", fig.align='center'}
plant_dag <- dagitty("dag{
      H_0 -> H_1 
      F -> H_1 
      T -> F
}")
coordinates( plant_dag ) <- list(x = c(H_0 = 0 , T = 2, F = 1.5, H_1 = 1), 
                                 y = c(H_0 = 0 , T = 0, F = 0, H_1 = 0))
drawdag(plant_dag)
```

如果我們錯誤地把 $F$ 也放入預測變量中去的話，就把實際治療變量的效果這條通路給堵住了。這在因果推斷中被叫做，由於控制了 F 變量，我們錯誤地在模型中引入了 D - separation。這裡的 D，指的是 directional（方向）。D-separation 在因果推斷中指的是，某個變量在DAG圖中和其他所有變量都獨立。在本例中， 由於控制了 $F$ 而導致從治療變量 $T$ 通往結果變量 $H_1$ 之間的的通路被阻斷了 ($T \rightarrow F \rightarrow H_1$)，使得 $H_1$ 和 $T$ 之間變得失去了依賴關係（相互獨立）。


事實上，錯誤地在預測變量中放入治療結果造成的結果不只是可能使我麼誤認為治療無效，也可能使我們誤認為原本無效的治療是有效的。看如下圖 \@ref(fig:introBayes09-fig08) 所提示的因果關係。它的涵義是，該治療土壤的方法確實導致了某些奇怪的菌落的生長，但是，我們種的那個植物並不會被菌落的生長所影響。但是假設有一個未知未測量的變量 "M"，它會同時影響植物和菌落的生長（例如空氣濕度）。這時如果我們建立一個簡單線型回歸模型來尋找治療 $T$ 和植物生長 $H_1$ 之間的關係的話，不小心加入了菌落這一變量會導致本來沒有關係的二者突然出現了治療效果一樣的聯繫。我們來試著模擬一下這個現象。


```{r  introBayes09-fig08, cache=TRUE, fig.width=8, fig.height=2,  fig.cap="The other DAG of the fungus and treatment effect on the grow of plant.", fig.align='center'}
# define our coordinates
dag_coords <-
  tibble(name = c("H0", "H1", "M", "F", "T"),
         x    = c(1, 2, 2.5, 3, 4),
         y    = c(2, 2, 1, 2, 2))

# save our DAG
dag <-
  dagify(F ~ M + T,
         H1 ~ H0 + M,
         coords = dag_coords)

# plot 
dag %>%
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(aes(color = name == "M"),
                 alpha = 1/2, size = 6.5, show.legend = F) +
  geom_point(x = 2.5, y = 1, 
             size = 6.5, shape = 1, stroke = 1, color = "orange") +
  geom_dag_text(color = "black") +
  geom_dag_edges() + 
  scale_color_manual(values = c("steelblue", "orange")) +
  theme_dag()
```

```{r introBayes09-11, cache=TRUE}
set.seed(71)
N <- 1000
h0 <- rnorm(N, 10, 2)
treatment <- rep(0:1, each = N/2)
M <- rbern(N)
fungus <- rbinom( N, size = 1, prob = 0.5 - treatment * 0.5 + 0.4 * M)
h1 <- h0 + rnorm( N, 5 + 3 * M)
d2 <- data.frame( h0 = h0, h1 = h1, treatment = treatment, fungus = fungus)
precis(d2)
# incorrectly included fugus 
m6.7 <- quap(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- h0 * p , 
    p <- a + bt * treatment + bf * fungus, 
    a ~ dlnorm( 0, 0.2 ),
    bt  ~ dnorm(0, 0.5), 
    bf  ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d2
)
precis(m6.7)

# the correct model to see the treatment effect 
m6.8 <- quap(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- h0 * p, 
    p <- a + bt * treatment, 
    a ~ dlnorm(0, 0.25),
    bt ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d2
)
precis(m6.8)
```

此時你發現加入 `fungus` 變量依然對正確的對斷造成了干擾。使得本來不應該出現的治療效果似乎突然成了有效的促進植物生長的治療。

## 對撞因子偏倚 collider bias

使用本章節開頭的申請研究經費的例子，我們認為研究的可靠性 (Trustworthiness, T)，和新穎程度 (Newsworthiness, N) 之間是無關聯性的（圖 \@ref(fig:introBayes09-fig01)）。但是，他們二者都會對是否該科研項目被選中 (Selected, S) 造成影響。這樣的關係可以使用下面的 DAG 來表達：

```{r  introBayes09-fig09, cache=TRUE, fig.width=8, fig.height=1,  fig.cap="The DAG of the grant selection problem: two unrelated variables (T and N) influence S, a collider example.", fig.align='center'}
grant_dag <- dagitty("dag{
      T -> S
      N -> S
}")
coordinates( grant_dag ) <- list(x = c(T = 0.5, S = 1, N = 1.5), 
                                 y = c(T = 0, S = 0, N = 0))
drawdag(grant_dag)
```


對撞因子偏倚的現象很有趣，當上述模型中加入對撞因子 S，就會在統計學上給出影響該對撞因子的變量之間的錯誤的關聯性，這裡就是本不該有關聯的 N 和 T 之間會出現統計學上的關聯性。因為，從邏輯上來說，當你知道了某個項目被選中了，也就是圖 \@ref(fig:introBayes09-fig01) 中藍色的部分，那麼本來不相關的兩個變量之間就存在了互相可以預測的掛係，即，如果此時你又對該科研項目的可信度或者是新穎度之一有所了解的話，你就可以大致猜測它的新穎度或者是可信度。也就是說，在這些被選中接受科研經費贊助的藍色項目中，如果你知道某項目的新穎程度很高很高，那麼你大概可以認為它給出的科研成果的可信度會比較低。同樣的，如果你知道某個科研項目並不是特別新穎的內容，但是它既然被選中了，這就說明該項目本身將會給出的科研成果會是十分令人信服的。

### 虛假的傷心對撞因子 collider of false sorrow

思考年齡和信服感之間的關係。年齡是否會和幸福感有關係呢？如果有關係，他們之間的關係能算作是因果關係嗎？這裡我們大膽假定每個人出生時幸福感就已被定格，不會隨著年齡而變化。我們已知幸福感會影響一個人是否結婚的概率，大概天天比較樂觀開心表現的有幸福感的人，結婚的概率也相對高一些。另一個可能影響結婚與否的變量一般認為是年齡。很顯然，存活的時間越長，越有機會結婚。這三者之間的關係類似地也可以表達成為 DAG 因果關係圖：


```{r introBayes09-fig10, cache=TRUE, fig.width=8, fig.height=1,  fig.cap="The DAG of the happiniess problem: two unrelated variables (H and A) influence marriage.", fig.align='center'}

marriage_dag <- dagitty("dag{
      H -> M
      A -> M
}")
coordinates( marriage_dag ) <- list(x = c(H = 0.5, M = 1, A = 1.5), 
                                 y = c(H = 0, M = 0, A = 0))
drawdag(marriage_dag)
```

根據我們理解的理論，年齡和幸福感各自都會影響結婚與否。結婚這個變量就是一個對撞因子 (collider)。即使我們知道年齡和幸福感之間不應該存在直接的關係，但是假如我們有一個模型，結果變量是幸福感，預測變量是年齡（或者反過來）的話，在預測變量裡增加結婚這個變量會導致本來沒有關係的二者變得有“統計學關係”。這就顯然會誤導我們認為年齡增加和幸福感的增加或者減少是有關聯的（而事實上應該是無關的）。

我們用一個較為極端的例子來做一次計算機模擬：

1. 每年有20名實驗對象出生，且他們擁有符合均一分佈特徵的幸福感。
2. 每年，實驗對象年齡自然會增加一歲。然而幸福感並不會因年齡的增加而增加或減少。
3. 當實驗對象18歲時，有些人會結婚。結婚本身的比值 (odds) 則於該實驗對象的幸福感成一定的比例關係 (proportional)。
4. 當一名實驗對象結婚了以後，她/他保持結婚的狀態，不會離婚。
5. 年齡到65歲之後，該實驗對象離開本次研究。


```{r introBayes09-12, cache=TRUE}
d <- sim_happiness( seed = 1977, N_years = 1000)
precis(d)
```

這個實驗性的計算機模擬數據本身包含了0-65歲的1300名實驗對象的幸福感和結婚與否的數據。

```{r introBayes09-fig11, cache=TRUE, fig.width=8, fig.height=4,  fig.cap="Simulated data, assuming that happiness is uniformly distributed and never changes. Each point is a person. Married individuals are shown with filled blue points. At each age after 18, the happiest individuals are more likely to be married. At later ages, more individuals tend to be married. Marriage status is a collider of age and happiness: A -> M <- H. If we condition on marriage in a regression, it will mislead us to believe that happiness declines with age.", fig.align='center'}
d %>% 
  mutate(married = factor(married,
                          labels = c("unmarried", "married"))) %>% 
  
  ggplot(aes(x = age, y = happiness)) +
  geom_point(aes(color = married), size = 1.75) +
  scale_color_manual(NULL, values = c("grey85", "forestgreen")) +
  scale_x_continuous(expand = c(.015, .015)) +
  theme(panel.grid = element_blank())
```

這時，我們希望用這個數據來回答：“年齡是否和幸福感有關係？”這樣的問題。假設你不知道我們在生成這組數據時遵循的上述 1 - 5 條原則。所以你在建立模型的時候很可能自然而然的認為婚姻本身是年齡和幸福感之間關係的混雜因子。也就是你大概會認為結婚的人莫名其妙地就應該比相對更加（不）幸福。這樣的模型應該是這樣子的；


$$
\mu_i = \alpha_{\text{MID}[i]} + \beta_A A_i
$$

其中，

- $\text{MID}[i]$ 是實驗對象 $i$ 是否已經結婚的索引變量 (index variable)，當它等於1時表示單身，等於2時表示已婚。
- 這其實是我們人為地給已婚者和單身者的幸福感和年齡之間關係的直線設定了各自的截距。

這時，由於18歲以後才可以結婚，我們把該數據的人口年齡限定在18歲及以上者。另外我們再把年齡的尺度縮放一下使得 18-65 歲之間的比例是1：

```{r introBayes09-13, cache=TRUE}
d2 <- d[ d$age > 17, ] # adults only
d2$A <- (d2$age - 18) / (65 - 18 )
```

經過上述的數據處理，我們使得年齡變量 A 的範圍控制在 0-1 之間，其中 0 代表 18 歲，1 代表 65 歲。幸福感則是一個範圍在 -2, 2 之間的數值。這樣的話，假定年齡和幸福感之間呈現的是即強烈的正關係，那麼這最極端的斜率也就是 $(2 - (-2)) / 1 = 4$。所以，一個較為合理的斜率的先驗概率分佈，可以是95%的斜率取值分佈在小於極端斜率之內的範圍。其次是為截距 $\alpha$ 設定合理的先驗概率分佈。因為 $\alpha$ 本身是年齡等於零，也就是18歲時的幸福感，我們需要這個數據能夠覆蓋所有可能的幸福感取值，-2，2 之間。那麼，標準正（常）態分佈是一個不錯的選擇。

```{r introBayes09-14, cache=TRUE}
d2$mid <- d2$married + 1 # construct the marriage status index variable
m6.9 <- quap(
  alist(
    happiness ~ dnorm(mu, sigma), 
    mu <- a[mid] + bA * A, 
    a[mid] ~ dnorm(0, 1), 
    bA ~ dnorm(0, 2), 
    sigma ~ dexp(1)
  ), data = d2
)
precis(m6.9, depth = 2)
```

看，這個模型似乎很確定，年齡和幸福感是呈現負關係的。對比一下沒有假如婚姻狀態的變量的模型給出的估計結果：


```{r introBayes09-15, cache=TRUE}
m6.10 <- quap(
  alist(
    happiness ~ dnorm(mu, sigma), 
    mu <- a + bA * A, 
    a ~ dnorm( 0, 1 ), 
    bA ~ dnorm(0, 2),
    sigma ~ dexp(1)
  ), data = d2
)
precis(m6.10)
```

`m6.10` 才是正確的模型。它正確的給出了年齡和幸福感之間並無關係的結果。模型 `m6.9` 錯誤地把對撞因子 -- 婚姻狀況作為預測變量之一加入了模型中，而婚姻狀況在這個數據背景下，同時是年齡和幸福感的結果（common consequence of age and happiness）。結果就會像 `m6.9` 那樣，出現年齡和幸福感之間虛假的負關係 (false negative association between the two causes)。`m6.9` 告訴我們看起來似乎年齡的增長和幸福感呈現負關係，這在模型中給出的關係應該嚴格來說只能算是一種統計學的關係，不能算是真實的因果關係。正如圖 \@ref(fig: introBayes09-fig11) 所顯示的。當已知實驗對象是已婚或者未婚，實驗對象的年齡似乎能告訴我們他/她的幸福度。看綠色點的部分，這些人都是已婚者，年齡越大，越多人結婚，那麼這個已婚人群的幸福度數值就會平均被拉低。相似的，看空白點的部分，這些人都是未婚者，年齡越大，其中幸福感較強的人都結婚而加入了已婚人群陣營，那麼剩下的人就會感覺幸福度越來越低。所以，當把人群分成未婚和已婚兩個部分的話，這兩個人群中的幸福度都隨著年齡增加而呈現下降趨勢。但是，我們知道，這並不是真實的因果關係。


碰撞因子偏倚本身會出現在當模型中的預測變量加入了某個同時是結果和某一個預測變量的結果的變量（common consequence）。


### 對撞因子偏倚另一實例（未測量變量造成的碰撞偏倚）


